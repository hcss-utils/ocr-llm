# -*- coding: utf-8 -*-
"""Deepseek_OCR_(3B).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Deepseek_OCR_(3B).ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os, re
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     import torch; v = re.match(r"[0-9\.]{3,}", str(torch.__version__)).group(0)
#     xformers = "xformers==" + ("0.0.32.post2" if v == "2.8.0" else "0.0.29.post3")
#     !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
#     !pip install --no-deps unsloth
# !pip install transformers==4.56.2
# !pip install --no-deps trl==0.22.2
# !pip install jiwer
# !pip install einops addict easydict

"""### Unsloth

Let's prepare the OCR model to our local first
"""

from huggingface_hub import snapshot_download

snapshot_download("unsloth/DeepSeek-OCR", local_dir="deepseek_ocr")

from unsloth import FastVisionModel  # FastLanguageModel for LLMs
import torch
from transformers import AutoModel
import os

os.environ["UNSLOTH_WARN_UNINITIALIZED"] = "0"
# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Qwen3-VL-8B-Instruct-bnb-4bit",  # Qwen 3 vision support
    "unsloth/Qwen3-VL-8B-Thinking-bnb-4bit",
    "unsloth/Qwen3-VL-32B-Instruct-bnb-4bit",
    "unsloth/Qwen3-VL-32B-Thinking-bnb-4bit",
]  # More models at https://huggingface.co/unsloth

model, tokenizer = FastVisionModel.from_pretrained(
    "./deepseek_ocr",
    load_in_4bit=False,  # Use 4bit to reduce memory use. False for 16bit LoRA.
    auto_model=AutoModel,
    trust_remote_code=True,
    unsloth_force_compile=True,
    use_gradient_checkpointing="unsloth",  # True or "unsloth" for long context
)

"""<a name="Inference"></a>
### Inference
Let's run the model!
"""

# !git clone https://github.com/hcss-utils/ocr-llm.git

from pathlib import Path

input_path = Path("data").resolve()
output_path = Path("outputs").resolve()

# !pip install -q pymupdf

import pymupdf

prompt = "<image>\nFree OCR. "


for filename in input_path.rglob("*.pdf"):
    doc = pymupdf.open(filename)
    results = []
    for page in doc:
        pix = page.get_pixmap()  # render page to an image
        output_image = f"data/{filename.stem}-page-{page.number}.png"
        pix.save(output_image)
        res = model.infer(
            tokenizer,
            prompt=prompt,
            image_file=output_image,
            output_path="output/deepseek",
            image_size=512,
            base_size=512,
            crop_mode=False,
            save_results=True,
            test_compress=False,
        )
        with open("output/deepseek/result.mmd", "r") as f:
            md = f.read()
        results.append(md)
    with open(output_path / f"deepseek-{filename.stem}.md", "w", encoding="utf-8") as f:
        f.write("\n\n".join(results))

# !zip -r deepseek.zip ocr-llm/outputs

# from google.colab import files

# files.download("deepseek.zip")
